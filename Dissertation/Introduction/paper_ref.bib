@inproceedings{7472778,
  author    = {X. {Xiao} and S. {Watanabe} and H. {Erdogan} and L. {Lu} and J. {Hershey} and M. L. {Seltzer} and G. {Chen} and Y. {Zhang} and M. {Mandel} and D. {Yu}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Deep beamforming networks for multi-channel speech recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {5745-5749},
  doi       = {10.1109/ICASSP.2016.7472778}
}

@inproceedings{7952160,
  author    = {Z. {Meng} and S. {Watanabe} and J. R. {Hershey} and H. {Erdogan}},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Deep long short-term memory adaptive beamforming networks for multichannel robust speech recognition},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {271-275},
  doi       = {10.1109/ICASSP.2017.7952160}
}

@article{8466865,
  author  = {W. {Jiang} and F. {Wen} and P. {Liu}},
  journal = {IEEE Access},
  title   = {Robust Beamforming for Speech Recognition Using DNN-Based Time-Frequency Masks Estimation},
  year    = {2018},
  volume  = {6},
  number  = {},
  pages   = {52385-52392},
  doi     = {10.1109/ACCESS.2018.2870758}
}

@article{9064910,
  author  = {H. {Taherian} and Z. {Wang} and J. {Chang} and D. {Wang}},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title   = {Robust Speaker Recognition Based on Single-Channel and Multi-Channel Speech Enhancement},
  year    = {2020},
  volume  = {28},
  number  = {},
  pages   = {1293-1302},
  doi     = {10.1109/TASLP.2020.2986896}
}

@inproceedings{4036866,
  author    = {J. W. {Stokes} and J. C. {Platt} and S. {Basu}},
  booktitle = {2006 IEEE International Conference on Multimedia and Expo},
  title     = {Speaker Identification using a Microphone Array and a Joint HMM with Speech Spectrum and Angle of Arrival},
  year      = {2006},
  volume    = {},
  number    = {},
  pages     = {1381-1384},
  doi       = {10.1109/ICME.2006.262796}
}

@inproceedings{674412,
  author    = {E. {Lleida} and J. {Fernandez} and E. {Masgrau}},
  booktitle = {Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)},
  title     = {Robust continuous speech recognition system based on a microphone array},
  year      = {1998},
  volume    = {1},
  number    = {},
  pages     = {241-244 vol.1},
  doi       = {10.1109/ICASSP.1998.674412}
}

@inproceedings{542818,
  author    = {A. {Wang} and K. {Yao} and R. E. {Hudson} and D. {Korompis} and F. {Lorenzellii} and S. {Soli} and S. {Gao}},
  booktitle = {Proceedings of International Conference on Application Specific Systems, Architectures and Processors: ASAP '96},
  title     = {Microphone array for hearing aid and speech enhancement applications},
  year      = {1996},
  volume    = {},
  number    = {},
  pages     = {231-239},
  doi       = {10.1109/ASAP.1996.542818}
}

@inproceedings{7471664,
  author    = {J. {Heymann} and L. {Drude} and R. {Haeb-Umbach}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Neural network based spectral mask estimation for acoustic beamforming},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {196-200},
  doi       = {10.1109/ICASSP.2016.7471664}
}

@inproceedings{9003849,
  author    = {Y. {Luo} and C. {Han} and N. {Mesgarani} and E. {Ceolini} and S. {Liu}},
  booktitle = {2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  title     = {FaSNet: Low-Latency Adaptive Beamforming for Multi-Microphone Audio Processing},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {260-267},
  doi       = {10.1109/ASRU46091.2019.9003849}
}

@misc{20202222222,
  title         = {Exploring End-to-End Multi-channel ASR with Bias Information for Meeting Transcription},
  author        = {Xiaofei Wang and Naoyuki Kanda and Yashesh Gaur and Zhuo Chen and Zhong Meng and Takuya Yoshioka},
  year          = {2020},
  eprint        = {2011.03110},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@misc{900384911,
  title         = {On the Comparison of Popular End to End Models for Large Scale Speech Recognition},
  author        = {Jinyu Li and Yu Wu and Yashesh Gaur and Chengyi Wang and Rui Zhao and Shujie Liu},
  year          = {2020},
  eprint        = {2011.03110},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@article{6296526,
  author  = {G. {Hinton} and L. {Deng} and D. {Yu} and G. E. {Dahl} and A. {Mohamed} and N. {Jaitly} and A. {Senior} and V. {Vanhoucke} and P. {Nguyen} and T. N. {Sainath} and B. {Kingsbury}},
  journal = {IEEE Signal Processing Magazine},
  title   = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
  year    = {2012},
  volume  = {29},
  number  = {6},
  pages   = {82-97},
  doi     = {10.1109/MSP.2012.2205597}
}


@inproceedings{thomas2016bfgui,
  author    = {Thomas, Mark and Gamper, Hannes and Tashev, Ivan},
  title     = {BFGUI: An Interactive Tool for the Synthesis and Analysis of Microphone Array Beamformers},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year      = {2016},
  month     = {March},
  abstract  = {Microphone arrays are beneficial for distant speech capture because the signals they capture can be exploited with beamforming to suppress noise and reverberation. The theory for the design and analysis of microphone arrays is well established, however the performance of a microphone array beamformer is often subject to conflicting criteria that need to be assessed manually. This paper describes BFGUI, a interactive graphical tool forMATLAB, for simulating microphone arrays and synthesizing beamformers, and whose parameters can be modified and performance metrics monitored in real-time. Primarily aimed at teaching and research, this tool provides the user with an intuitive insight into the effects of microphone types, number and geometry, and the influence of design constraints such as regularization and white noise gain on derived metrics. The resulting directivity pattern, directivity index and front-back ratio are examples of such metrics. Multiple analytic microphone models are supported and external measured microphone directivity patterns can also be loaded. The designs can be then exported in a variety of formats for processing of real-world data.},
  publisher = {IEEE - Institute of Electrical and Electronics Engineers},
  url       = {https://www.microsoft.com/en-us/research/publication/bfgui-an-interactive-tool-for-the-synthesis-and-analysis-of-microphone-array-beamformers/}
}

@inproceedings{gamper2020blind,
  author       = {Gamper, Hannes},
  title        = {Blind C50 estimation from single-channel speech using a convolutional neural network},
  organization = {IEEE},
  booktitle    = {Proc. IEEE Int. Workshop Multimedia Signal Processing (MMSP)},
  year         = {2020},
  month        = {September},
  abstract     = {The early-to-late reverberation energy ratio is an important parameter describing the acoustic properties of an environment. C50, i.e., the ratio between the first 50 ms and the remaining late energy, affects the perceived clarity and intelligibility of speech, and can be used as a design parameter in mixed reality applications or to predict the performance of speech recognition systems. While established methods exist to derive C50 from impulse response measurements, such measurements are rarely available in practice. Recently, methods have been proposed to estimate C50 blindly from reverberant speech signals. Here, a convolutional neural network (CNN) architecture with a long short-term memory (LSTM) layer is proposed to estimate C50 blindly. The CNN-LSTM operates directly on the spectrogram of variable-length, noisy, reverberant utterances. A feature comparison indicates that log Mel spectrogram features with a frame size of 128 samples achieve the best performance with an average root-mean-square error of about 2.7 dB, outperforming previously proposed blind C50 estimators.},
  url          = {https://www.microsoft.com/en-us/research/publication/blind-c50-estimation-from-single-channel-speech-using-a-convolutional-neural-network/}
}

@inproceedings{gamper2020predicting,
  author       = {Gamper, Hannes and Emmanouilidou, Dimitra and Braun, Sebastian and Tashev, Ivan},
  title        = {Predicting Word Error Rate for Reverberant Speech},
  organization = {IEEE},
  booktitle    = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year         = {2020},
  month        = {May},
  abstract     = {Reverberation negatively impacts the performance of automatic speech recognition (ASR). Prior work on quantifying the effect of reverberation has shown that clarity (C50), a parameter that can be estimated from the acoustic impulse response, is correlated with ASR performance. In this paper we propose predicting ASR performance in terms of the word error rate (WER) directly from acoustic parameters via a polynomial, sigmoidal, or neural network fit, as well as blindly from reverberant speech samples using a convolutional neural network (CNN). We carry out experiments on two state-of-the-art ASR models and a large set of acoustic impulse responses (AIRs). The results confirm C50 and C80 to be highly correlated with WER, allowing WER to be predicted with the proposed fitting approaches. The proposed non-intrusive CNN model outperforms C50-based WER prediction, indicating that WER can be estimated blindly, i.e., directly from the reverberant speech samples without knowledge of the acoustic parameters.},
  url          = {https://www.microsoft.com/en-us/research/publication/predicting-word-error-rate-for-reverberant-speech/},
  pages        = {491-495}
}

@inproceedings{xia2020weighted,
  author       = {Xia, Yangyang and Braun, Sebastian and Reddy,  Chandan K. A. and Dubey, Harishchandra and Cutler, Ross and Tashev, Ivan},
  title        = {Weighted Speech Distortion Losses for Neural-Network-Based Real-Time Speech Enhancement},
  organization = {IEEE},
  booktitle    = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year         = {2020},
  month        = {May},
  abstract     = {This paper investigates several aspects of training a RNN (recurrent neural network) that impact the objective and subjective quality of enhanced speech for real-time single-channel speech enhancement. Specifically, we focus on a RNN that enhances short-time speech spectra on a single-frame-in, single-frame-out basis, a framework adopted by most classical signal processing methods. We propose two novel mean-squared-error-based learning objectives that enable separate control over the importance of speech distortion versus noise reduction. The proposed loss functions are evaluated by widely accepted objective quality and intelligibility measures and compared to other competitive online methods. In addition, we study the impact of feature normalization and varying batch sequence lengths on the objective quality of enhanced speech. Finally, we show subjective ratings for the proposed approach and a state-of-the-art real-time RNN-based method.},
  url          = {https://www.microsoft.com/en-us/research/publication/weighted-speech-distortion-losses-for-neural-network-based-real-time-speech-enhancement/},
  pages        = {871-875}
}

@inproceedings{chen2018efficient,
  author    = {Chen, Zhuo and Yoshioka, Takuya and Xiao, Xiong and Li, Jinyu and Seltzer, Michael L. and Gong, Yifan},
  title     = {Efficient integration of fixed beamformers and speech separation networks for multi-channel far-field speech separation},
  booktitle = {ICASSP},
  year      = {2018},
  month     = {April},
  abstract  = {Speech separation research has significantly progressed in recent years thanks to the rapid advances in deep learning technology.
However the performance of recently proposed single-channel neural network-based speech separation methods is still limited especially in reverberant environments.
To push the performance limit,
we recently developed a method of integrating
beamforming and single-channel speech separation approaches.
This paper proposes a novel architecture that integrates
multi-channel beamforming and speech separation in
a much more efficient way  than our previous method.
The proposed architecture comprises
a set of fixed beamformers,
a beam prediction network,
and a speech separation network based on permutation
invariant training (PIT).
The beam prediction network takes in the beamformed audio signals
and estimates the best beam for each speaker constituting the input mixture.
Two variants of PIT-based speech separation networks are proposed.
Our approach is evaluated on reverberant speech mixtures
under three different mixing conditions, covering cases
where speakers partially overlap or one speaker's utterance is very short.
The experimental results show that the proposed system
significantly outperforms the conventional single-channel PIT system, producing the same performance as a single-channel system using oracle masks.},
  publisher = {IEEE},
  url       = {https://www.microsoft.com/en-us/research/publication/efficient-integration-fixed-beamformers-speech-separation-networks-multi-channel-far-field-speech-separation-2/},
  edition   = {ICASSP}
}

@inproceedings{boeddeker2018exploring,
  author    = {Boeddeker, Christoph and Erdogan, Hakan and Yoshioka, Takuya and Haeb-Umbach, Reinhold},
  title     = {Exploring Practical Aspects of Neural Mask-Based Beamforming for Far-Field Speech Recognition},
  booktitle = {ICASSP 2018},
  year      = {2018},
  month     = {April},
  abstract  = {This work examines acoustic beamformers employing neural networks (NNs) for mask prediction as front-end for automatic speech recognition (ASR) systems for practical scenarios like voice-enabled home devices. To test the versatility of the mask predicting network, the system is evaluated with different recording hardware, different microphone array designs, and different acoustic models of the downstream ASR system. Significant gains in recognition accuracy
are obtained in all configurations despite the fact that the NN had been trained on mismatched data. Unlike previous work, the NN is trained on a feature level objective, which gives some performance advantage over a mask related criterion. Furthermore, different approaches for realizing online, or adaptive, NN-based beamforming are explored, where the online algorithms still show significant gains compared to the baseline performance.},
  publisher = {IEEE},
  url       = {https://www.microsoft.com/en-us/research/publication/exploring-practical-aspects-neural-mask-based-beamforming-far-field-speech-recognition/},
  edition   = {ICASSP 2018}
}

@inproceedings{inproceedings,
  author = {Lleida, Eduardo and Navajas, Julian and Masgrau, Enrique},
  year   = {1998},
  month  = {06},
  pages  = {241 - 244 vol.1},
  title  = {Robust continuous speech recognition system based on a microphone array},
  volume = {1},
  isbn   = {0-7803-4428-6},
  doi    = {10.1109/ICASSP.1998.674412}
}

@inproceedings{7472671,
  author    = {T. {Higuchi} and N. {Ito} and T. {Yoshioka} and T. {Nakatani}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Robust MVDR beamforming using time-frequency masks for online/offline ASR in noise},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {5210-5214},
  doi       = {10.1109/ICASSP.2016.7472671}
}

@article{Wang2020SemanticMF,
  title   = {Semantic Mask for Transformer based End-to-End Speech Recognition},
  author  = {Chengyi Wang and Y. Wu and Yujiao Du and J. Li and Shujie Liu and Liang Lu and Shuo Ren and Guoli Ye and Sheng Zhao and M. Zhou},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/1912.03010}
}

@online{kaldiGit,
  author = {},
  title  = {Kaldi Speech Recognition Toolkit},
  year   = 2016,
  url    = {https://github.com/kaldi-asr/kaldi }
}

@online{espnetGit,
  author = {},
  title  = {ESPnet, end-to-end speech processing toolkit},
  year   = 2017,
  url    = {https://espnet.github.io/espnet/ }
}

@online{pykaldiGit,
  author = {},
  title  = {PyKaldi, Python wrapper for Kaldi},
  year   = 2017,
  url    = {https://pykaldi.github.io/ }
}

@online{pyAudioAnalysisGit,
  author = {},
  title  = {pyAudioAnalysis,  A Python library for audio feature extraction, classification, segmentation and applications},
  year   = 2017,
  url    = {https://github.com/tyiannak/pyAudioAnalysis }
}

@online{setkGit,
  author = {},
  title  = {setk,  Toolset for Speech Enhancement integrated with Kaldi},
  year   = 2017,
  url    = {https://github.com/funcwj/setk }
}

@article{asrBriefHistory,
  author = {Juang, B. and Rabiner, Lawrence},
  year   = {2005},
  month  = {01},
  pages  = {},
  title  = {Automatic Speech Recognition - A Brief History of the Technology Development}
}

@inproceedings{7472621,
  author    = {W. {Chan} and N. {Jaitly} and Q. {Le} and O. {Vinyals}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Listen, attend and spell: A neural network for large vocabulary conversational speech recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {4960-4964},
  doi       = {10.1109/ICASSP.2016.7472621}
}

@article{hannun2017sequence,
  author  = {Hannun, Awni},
  title   = {Sequence Modeling with CTC},
  journal = {Distill},
  year    = {2017},
  note    = {https://distill.pub/2017/ctc},
  doi     = {10.23915/distill.00008}
}

https://github.com/kaldi-asr/kaldi
https://espnet.github.io/espnet/
https://pykaldi.github.io/
https://github.com/tyiannak/pyAudioAnalysis
https://github.com/funcwj/setk