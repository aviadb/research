@article{BROWN1994297,
title = {Computational auditory scene analysis},
journal = {Computer Speech \& Language},
volume = {8},
number = {4},
pages = {297-336},
year = {1994},
issn = {0885-2308},
doi = {https://doi.org/10.1006/csla.1994.1016},
url = {https://www.sciencedirect.com/science/article/pii/S0885230884710163},
author = {Guy J. Brown and Martin Cooke},
abstract = {Although the ability of human listeners to perceptually segregate concurrent sounds is well documented in the literature, there have been few attempts to exploit this research in the design of computational systems for sound source segregation. In this paper, we present a segregation system that is consistent with psychological and physiological findings. The system is able to segregate speech from a variety of intrusive sounds, including other speech, with some success. The segregation system consists of four stages. Firstly, the auditory periphery is modelled by a bank of bandpass filters and a simulation of neuromechanical transduction by inner hair cells. In the second stage of the system, periodicities, frequency transitions, onsets and offsets in auditory nerve firing patterns are made explicit by separate auditory representations. The representations, auditory maps, are based on the known topographical organization of the higher auditory pathways. Information from the auditory maps is used to construct a symbolic description of the auditory scene. Specifically, the acoustic input is characterized as a collection of time-frequency elements, each of which describes the movement of a spectral peak in time and frequency. In the final stage of the system, a search strategy is employed which groups elements according to the similarity of their fundamental frequencies, onset times and offset times. Following the search, a waveform can be resynthesized from a group of elements so that segregation performance may be assessed by informal listening tests. The system has been evaluated using a database of voiced speech mixed with a variety of intrusive noises such as music, "office" noise and other speech. A technique for quantitative evaluation of the system is described, in which the signal-to-noise ratio (SNR) is compared before and after the segregation process. After segregation, an increase in SNR is obtained for each noise condition. Additionally, the performance of our system is significantly better than that of the frame-based segregation scheme described by Meddis and Hewitt (1992).}
}

@INPROCEEDINGS{6709849,  author={Pal, Madhab and Roy, Rajib and Basu, Joyanta and Bepari, Milton S.},  booktitle={2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)},   title={Blind source separation: A review and analysis},   year={2013},  volume={},  number={},  pages={1-5},  doi={10.1109/ICSDA.2013.6709849}}

@article{Xia2017UsingOR,
  title={Using optimal ratio mask as training target for supervised speech separation},
  author={Shasha Xia and Hao Li and Xueliang Zhang},
  journal={2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  year={2017},
  pages={163-166}
}

@article{7364200,  author={Williamson, Donald S. and Wang, Yuxuan and Wang, DeLiang},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={Complex Ratio Masking for Monaural Speech Separation},   year={2016},  volume={24},  number={3},  pages={483-492},  doi={10.1109/TASLP.2015.2512042}}

@article{Jiang2018RobustBF,
  title={Robust Beamforming for Speech Recognition Using DNN-Based Time-Frequency Masks Estimation},
  author={Wenbin Jiang and Fei Wen and Peilin Liu},
  journal={IEEE Access},
  year={2018},
  volume={6},
  pages={52385-52392}
}