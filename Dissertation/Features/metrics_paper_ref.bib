@INPROCEEDINGS{941023,

  author={Rix, A.W. and Beerends, J.G. and Hollier, M.P. and Hekstra, A.P.},

  booktitle={2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)}, 

  title={Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs}, 

  year={2001},

  volume={2},

  number={},

  pages={749-752 vol.2},

  doi={10.1109/ICASSP.2001.941023}}

@INPROCEEDINGS{5495701,

  author={Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},

  booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing}, 

  title={A short-time objective intelligibility measure for time-frequency weighted noisy speech}, 

  year={2010},

  volume={},

  number={},

  pages={4214-4217},

  doi={10.1109/ICASSP.2010.5495701}}

@ARTICLE{1643671,

  author={Vincent, E. and Gribonval, R. and Fevotte, C.},

  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 

  title={Performance measurement in blind audio source separation}, 

  year={2006},

  volume={14},

  number={4},

  pages={1462-1469},

  doi={10.1109/TSA.2005.858005}}

@article{1643671009,
author = {Liang, Shan and Liu, Wen-Ju and Jiang, Wei and Xue, Wei},
year = {2014},
month = {04},
pages = {22-30},
title = {The analysis of the simplification from the ideal ratio to binary mask in signal-to-noise ratio sense},
volume = {59},
journal = {Speech Communication},
doi = {10.1016/j.specom.2013.12.002}
}

@misc{roux2018sdr,
      title={SDR - half-baked or well done?}, 
      author={Jonathan Le Roux and Scott Wisdom and Hakan Erdogan and John R. Hershey},
      year={2018},
      eprint={1811.02508},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@phdthesis{10.5555/912256,
author = {Quackenbush, Schuyler Reynier},
title = {Objective Measures of Speech Quality (Subjective)},
year = {1985},
publisher = {Georgia Institute of Technology},
address = {USA},
abstract = {This thesis investigates objective measures of speech quality, or measures which can be computed from properties of an original and a distorted speech waveform. Two sets of objective measures are investigated. The first set is designed to estimate the specific types of perceived distortions specified by the 'parametric' subjective quality scales of the Diagnostic Acceptability Measure (DAM), a subjective speech quality test. The narrow scope of these parametric scales promotes a close coupling between physical quantities and their associated perceptual qualities, resulting in quite accurate measures. The second set of objective measures estimate the composite acceptability scale of the DAM, a scale measuring overall speech acceptability. The first set of measures are used as a foundation for designing the second set of measures. The speech quality data base used in this research is quite extensive. It consists of a total of 1056 examples of distorted speech produced by various speech coder and other speech distorting systems and their associated subjective quality ratings as produced by the Diagnostic Acceptability Measure.The thesis research can be divided into three parts. In the first part, the relationship between the scales of the Diagnostic Acceptability Measure and several of the best available objective speech quality measures is analyzed. It is demonstrated, using Multidimensional Scaling, that current objective measures produce poor estimates of composite acceptability because they incorporate little of the information provided by the parametric subjective measures. Multiple linear regression is used to find a linear relationship between the parametric subjective quality scales and the composite acceptability scale. The resulting regression model produces very good estimates of composite acceptability using only a subset of the parametric qualities. Because of this relationship, objective measures of parametric quality can be used as the basis for a measure of composite acceptability.In the second part, a set of objective measures is designed which provide dramatically improved estimates of the parametric qualities of the Diagnostic Acceptability Measure. Performance is measured in terms of the correlation between actual and estimated subjective quality. Multidimensional scaling graphically shows the remarkable ability of these measures to estimate the parametric subjective qualities.And finally, in the third part, the parametric objective measures are combined into a single composite measure for estimating subjective composite acceptability. Several measures are presented, with correlations to composite acceptability ranging from 0.81 to 0.85.},
note = {AAI8521249}
}

@article{_isword,
    author = {Wang, Y. and Acero, A. and Chelba, C.},
    title = {Is Word Error Rate a Good Indicator for Spoken Language Understanding Accuracy},
    year = {2003}
}

@article{KLAKOW200219,
title = {Testing the correlation of word error rate and perplexity},
journal = {Speech Communication},
author = {Dietrich Klakow and Jochen Peters},
keywords = {Language model training, Perplexity, Correlation with word error rate}
}

@inproceedings{7472778,
  author    = {X. {Xiao} and S. {Watanabe} and H. {Erdogan} and L. {Lu} and J. {Hershey} and M. L. {Seltzer} and G. {Chen} and Y. {Zhang} and M. {Mandel} and D. {Yu}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Deep beamforming networks for multi-channel speech recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {5745-5749},
  doi       = {10.1109/ICASSP.2016.7472778}
}

@inproceedings{7952160,
  author    = {Z. {Meng} and S. {Watanabe} and J. R. {Hershey} and H. {Erdogan}},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Deep long short-term memory adaptive beamforming networks for multichannel robust speech recognition},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {271-275},
  doi       = {10.1109/ICASSP.2017.7952160}
}

@article{8466865,
  author  = {W. {Jiang} and F. {Wen} and P. {Liu}},
  journal = {IEEE Access},
  title   = {Robust Beamforming for Speech Recognition Using DNN-Based Time-Frequency Masks Estimation},
  year    = {2018},
  volume  = {6},
  number  = {},
  pages   = {52385-52392},
  doi     = {10.1109/ACCESS.2018.2870758}
}

@article{9064910,
  author  = {H. {Taherian} and Z. {Wang} and J. {Chang} and D. {Wang}},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title   = {Robust Speaker Recognition Based on Single-Channel and Multi-Channel Speech Enhancement},
  year    = {2020},
  volume  = {28},
  number  = {},
  pages   = {1293-1302},
  doi     = {10.1109/TASLP.2020.2986896}
}

@inproceedings{4036866,
  author    = {J. W. {Stokes} and J. C. {Platt} and S. {Basu}},
  booktitle = {2006 IEEE International Conference on Multimedia and Expo},
  title     = {Speaker Identification using a Microphone Array and a Joint HMM with Speech Spectrum and Angle of Arrival},
  year      = {2006},
  volume    = {},
  number    = {},
  pages     = {1381-1384},
  doi       = {10.1109/ICME.2006.262796}
}

@inproceedings{674412,
  author    = {E. {Lleida} and J. {Fernandez} and E. {Masgrau}},
  booktitle = {Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)},
  title     = {Robust continuous speech recognition system based on a microphone array},
  year      = {1998},
  volume    = {1},
  number    = {},
  pages     = {241-244 vol.1},
  doi       = {10.1109/ICASSP.1998.674412}
}

@inproceedings{542818,
  author    = {A. {Wang} and K. {Yao} and R. E. {Hudson} and D. {Korompis} and F. {Lorenzellii} and S. {Soli} and S. {Gao}},
  booktitle = {Proceedings of International Conference on Application Specific Systems, Architectures and Processors: ASAP '96},
  title     = {Microphone array for hearing aid and speech enhancement applications},
  year      = {1996},
  volume    = {},
  number    = {},
  pages     = {231-239},
  doi       = {10.1109/ASAP.1996.542818}
}

@inproceedings{7471664,
  author    = {J. {Heymann} and L. {Drude} and R. {Haeb-Umbach}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Neural network based spectral mask estimation for acoustic beamforming},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {196-200},
  doi       = {10.1109/ICASSP.2016.7471664}
}

@inproceedings{9003849,
  author    = {Y. {Luo} and C. {Han} and N. {Mesgarani} and E. {Ceolini} and S. {Liu}},
  booktitle = {2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  title     = {FaSNet: Low-Latency Adaptive Beamforming for Multi-Microphone Audio Processing},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {260-267},
  doi       = {10.1109/ASRU46091.2019.9003849}
}

@misc{20202222222,
  title         = {Exploring End-to-End Multi-channel ASR with Bias Information for Meeting Transcription},
  author        = {Xiaofei Wang and Naoyuki Kanda and Yashesh Gaur and Zhuo Chen and Zhong Meng and Takuya Yoshioka},
  year          = {2020},
  eprint        = {2011.03110},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@misc{900384911,
  title         = {On the Comparison of Popular End to End Models for Large Scale Speech Recognition},
  author        = {Jinyu Li and Yu Wu and Yashesh Gaur and Chengyi Wang and Rui Zhao and Shujie Liu},
  year          = {2020},
  eprint        = {2011.03110},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@article{6296526,
  author  = {G. {Hinton} and L. {Deng} and D. {Yu} and G. E. {Dahl} and A. {Mohamed} and N. {Jaitly} and A. {Senior} and V. {Vanhoucke} and P. {Nguyen} and T. N. {Sainath} and B. {Kingsbury}},
  journal = {IEEE Signal Processing Magazine},
  title   = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
  year    = {2012},
  volume  = {29},
  number  = {6},
  pages   = {82-97},
  doi     = {10.1109/MSP.2012.2205597}
}


@inproceedings{thomas2016bfgui,
  author    = {Thomas, Mark and Gamper, Hannes and Tashev, Ivan},
  title     = {BFGUI: An Interactive Tool for the Synthesis and Analysis of Microphone Array Beamformers},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year      = {2016},
  month     = {March},
  abstract  = {Microphone arrays are beneficial for distant speech capture because the signals they capture can be exploited with beamforming to suppress noise and reverberation. The theory for the design and analysis of microphone arrays is well established, however the performance of a microphone array beamformer is often subject to conflicting criteria that need to be assessed manually. This paper describes BFGUI, a interactive graphical tool forMATLAB, for simulating microphone arrays and synthesizing beamformers, and whose parameters can be modified and performance metrics monitored in real-time. Primarily aimed at teaching and research, this tool provides the user with an intuitive insight into the effects of microphone types, number and geometry, and the influence of design constraints such as regularization and white noise gain on derived metrics. The resulting directivity pattern, directivity index and front-back ratio are examples of such metrics. Multiple analytic microphone models are supported and external measured microphone directivity patterns can also be loaded. The designs can be then exported in a variety of formats for processing of real-world data.},
  publisher = {IEEE - Institute of Electrical and Electronics Engineers},
  url       = {https://www.microsoft.com/en-us/research/publication/bfgui-an-interactive-tool-for-the-synthesis-and-analysis-of-microphone-array-beamformers/}
}

@inproceedings{gamper2020blind,
  author       = {Gamper, Hannes},
  title        = {Blind C50 estimation from single-channel speech using a convolutional neural network},
  organization = {IEEE},
  booktitle    = {Proc. IEEE Int. Workshop Multimedia Signal Processing (MMSP)},
  year         = {2020},
  month        = {September},
  abstract     = {The early-to-late reverberation energy ratio is an important parameter describing the acoustic properties of an environment. C50, i.e., the ratio between the first 50 ms and the remaining late energy, affects the perceived clarity and intelligibility of speech, and can be used as a design parameter in mixed reality applications or to predict the performance of speech recognition systems. While established methods exist to derive C50 from impulse response measurements, such measurements are rarely available in practice. Recently, methods have been proposed to estimate C50 blindly from reverberant speech signals. Here, a convolutional neural network (CNN) architecture with a long short-term memory (LSTM) layer is proposed to estimate C50 blindly. The CNN-LSTM operates directly on the spectrogram of variable-length, noisy, reverberant utterances. A feature comparison indicates that log Mel spectrogram features with a frame size of 128 samples achieve the best performance with an average root-mean-square error of about 2.7 dB, outperforming previously proposed blind C50 estimators.},
  url          = {https://www.microsoft.com/en-us/research/publication/blind-c50-estimation-from-single-channel-speech-using-a-convolutional-neural-network/}
}

@inproceedings{gamper2020predicting,
  author       = {Gamper, Hannes and Emmanouilidou, Dimitra and Braun, Sebastian and Tashev, Ivan},
  title        = {Predicting Word Error Rate for Reverberant Speech},
  organization = {IEEE},
  booktitle    = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year         = {2020},
  month        = {May},
  abstract     = {Reverberation negatively impacts the performance of automatic speech recognition (ASR). Prior work on quantifying the effect of reverberation has shown that clarity (C50), a parameter that can be estimated from the acoustic impulse response, is correlated with ASR performance. In this paper we propose predicting ASR performance in terms of the word error rate (WER) directly from acoustic parameters via a polynomial, sigmoidal, or neural network fit, as well as blindly from reverberant speech samples using a convolutional neural network (CNN). We carry out experiments on two state-of-the-art ASR models and a large set of acoustic impulse responses (AIRs). The results confirm C50 and C80 to be highly correlated with WER, allowing WER to be predicted with the proposed fitting approaches. The proposed non-intrusive CNN model outperforms C50-based WER prediction, indicating that WER can be estimated blindly, i.e., directly from the reverberant speech samples without knowledge of the acoustic parameters.},
  url          = {https://www.microsoft.com/en-us/research/publication/predicting-word-error-rate-for-reverberant-speech/},
  pages        = {491-495}
}

@inproceedings{xia2020weighted,
  author       = {Xia, Yangyang and Braun, Sebastian and Reddy,  Chandan K. A. and Dubey, Harishchandra and Cutler, Ross and Tashev, Ivan},
  title        = {Weighted Speech Distortion Losses for Neural-Network-Based Real-Time Speech Enhancement},
  organization = {IEEE},
  booktitle    = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year         = {2020},
  month        = {May},
  abstract     = {This paper investigates several aspects of training a RNN (recurrent neural network) that impact the objective and subjective quality of enhanced speech for real-time single-channel speech enhancement. Specifically, we focus on a RNN that enhances short-time speech spectra on a single-frame-in, single-frame-out basis, a framework adopted by most classical signal processing methods. We propose two novel mean-squared-error-based learning objectives that enable separate control over the importance of speech distortion versus noise reduction. The proposed loss functions are evaluated by widely accepted objective quality and intelligibility measures and compared to other competitive online methods. In addition, we study the impact of feature normalization and varying batch sequence lengths on the objective quality of enhanced speech. Finally, we show subjective ratings for the proposed approach and a state-of-the-art real-time RNN-based method.},
  url          = {https://www.microsoft.com/en-us/research/publication/weighted-speech-distortion-losses-for-neural-network-based-real-time-speech-enhancement/},
  pages        = {871-875}
}

@inproceedings{chen2018efficient,
  author    = {Chen, Zhuo and Yoshioka, Takuya and Xiao, Xiong and Li, Jinyu and Seltzer, Michael L. and Gong, Yifan},
  title     = {Efficient integration of fixed beamformers and speech separation networks for multi-channel far-field speech separation},
  booktitle = {ICASSP},
  year      = {2018},
  month     = {April},
  abstract  = {Speech separation research has significantly progressed in recent years thanks to the rapid advances in deep learning technology.
However the performance of recently proposed single-channel neural network-based speech separation methods is still limited especially in reverberant environments.
To push the performance limit,
we recently developed a method of integrating
beamforming and single-channel speech separation approaches.
This paper proposes a novel architecture that integrates
multi-channel beamforming and speech separation in
a much more efficient way  than our previous method.
The proposed architecture comprises
a set of fixed beamformers,
a beam prediction network,
and a speech separation network based on permutation
invariant training (PIT).
The beam prediction network takes in the beamformed audio signals
and estimates the best beam for each speaker constituting the input mixture.
Two variants of PIT-based speech separation networks are proposed.
Our approach is evaluated on reverberant speech mixtures
under three different mixing conditions, covering cases
where speakers partially overlap or one speaker's utterance is very short.
The experimental results show that the proposed system
significantly outperforms the conventional single-channel PIT system, producing the same performance as a single-channel system using oracle masks.},
  publisher = {IEEE},
  url       = {https://www.microsoft.com/en-us/research/publication/efficient-integration-fixed-beamformers-speech-separation-networks-multi-channel-far-field-speech-separation-2/},
  edition   = {ICASSP}
}

@inproceedings{boeddeker2018exploring,
  author    = {Boeddeker, Christoph and Erdogan, Hakan and Yoshioka, Takuya and Haeb-Umbach, Reinhold},
  title     = {Exploring Practical Aspects of Neural Mask-Based Beamforming for Far-Field Speech Recognition},
  booktitle = {ICASSP 2018},
  year      = {2018},
  month     = {April},
  abstract  = {This work examines acoustic beamformers employing neural networks (NNs) for mask prediction as front-end for automatic speech recognition (ASR) systems for practical scenarios like voice-enabled home devices. To test the versatility of the mask predicting network, the system is evaluated with different recording hardware, different microphone array designs, and different acoustic models of the downstream ASR system. Significant gains in recognition accuracy
are obtained in all configurations despite the fact that the NN had been trained on mismatched data. Unlike previous work, the NN is trained on a feature level objective, which gives some performance advantage over a mask related criterion. Furthermore, different approaches for realizing online, or adaptive, NN-based beamforming are explored, where the online algorithms still show significant gains compared to the baseline performance.},
  publisher = {IEEE},
  url       = {https://www.microsoft.com/en-us/research/publication/exploring-practical-aspects-neural-mask-based-beamforming-far-field-speech-recognition/},
  edition   = {ICASSP 2018}
}

@inproceedings{inproceedings,
  author = {Lleida, Eduardo and Navajas, Julian and Masgrau, Enrique},
  year   = {1998},
  month  = {06},
  pages  = {241 - 244 vol.1},
  title  = {Robust continuous speech recognition system based on a microphone array},
  volume = {1},
  isbn   = {0-7803-4428-6},
  doi    = {10.1109/ICASSP.1998.674412}
}

@inproceedings{7472671,
  author    = {T. {Higuchi} and N. {Ito} and T. {Yoshioka} and T. {Nakatani}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Robust MVDR beamforming using time-frequency masks for online/offline ASR in noise},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {5210-5214},
  doi       = {10.1109/ICASSP.2016.7472671}
}

@article{Wang2020SemanticMF,
  title   = {Semantic Mask for Transformer based End-to-End Speech Recognition},
  author  = {Chengyi Wang and Y. Wu and Yujiao Du and J. Li and Shujie Liu and Liang Lu and Shuo Ren and Guoli Ye and Sheng Zhao and M. Zhou},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/1912.03010}
}

@online{kaldiGit,
  author = {},
  title  = {Kaldi Speech Recognition Toolkit},
  year   = 2016,
  url    = {https://github.com/kaldi-asr/kaldi }
}

@online{espnetGit,
  author = {},
  title  = {ESPnet, end-to-end speech processing toolkit},
  year   = 2017,
  url    = {https://espnet.github.io/espnet/ }
}

@online{pykaldiGit,
  author = {},
  title  = {PyKaldi, Python wrapper for Kaldi},
  year   = 2017,
  url    = {https://pykaldi.github.io/ }
}

@online{pyAudioAnalysisGit,
  author = {},
  title  = {pyAudioAnalysis,  A Python library for audio feature extraction, classification, segmentation and applications},
  year   = 2017,
  url    = {https://github.com/tyiannak/pyAudioAnalysis }
}

@online{setkGit,
  author = {},
  title  = {setk,  Toolset for Speech Enhancement integrated with Kaldi},
  year   = 2017,
  url    = {https://github.com/funcwj/setk }
}

@article{asrBriefHistory,
  author = {Juang, B. and Rabiner, Lawrence},
  year   = {2005},
  month  = {01},
  pages  = {},
  title  = {Automatic Speech Recognition - A Brief History of the Technology Development}
}

@inproceedings{7472621,
  author    = {W. {Chan} and N. {Jaitly} and Q. {Le} and O. {Vinyals}},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Listen, attend and spell: A neural network for large vocabulary conversational speech recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {4960-4964},
  doi       = {10.1109/ICASSP.2016.7472621}
}

@article{hannun2017sequence,
  author  = {Hannun, Awni},
  title   = {Sequence Modeling with CTC},
  journal = {Distill},
  year    = {2017},
  note    = {https://distill.pub/2017/ctc},
  doi     = {10.23915/distill.00008}
}

@article{lowpowerSoc,
author = {Keating, M. and Flynn, David and Aitken, Rob and Gibbons, A. and Shi, K.},
year = {2007},
month = {01},
pages = {1-300},
title = {Low power methodology manual: For system-on-chip design},
doi = {10.1007/978-0-387-71819-4}
}


https://github.com/kaldi-asr/kaldi
https://espnet.github.io/espnet/
https://pykaldi.github.io/
https://github.com/tyiannak/pyAudioAnalysis
https://github.com/funcwj/setk